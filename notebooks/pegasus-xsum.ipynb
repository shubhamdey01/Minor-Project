{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets rouge_score nltk -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:19:55.879586Z","iopub.execute_input":"2024-11-07T16:19:55.880293Z","iopub.status.idle":"2024-11-07T16:20:08.166667Z","shell.execute_reply.started":"2024-11-07T16:19:55.880235Z","shell.execute_reply":"2024-11-07T16:20:08.165549Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # For more detailed CUDA error tracking\n\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom rouge_score import rouge_scorer\nimport nltk\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:20:08.168580Z","iopub.execute_input":"2024-11-07T16:20:08.168890Z","iopub.status.idle":"2024-11-07T16:20:12.715074Z","shell.execute_reply.started":"2024-11-07T16:20:08.168856Z","shell.execute_reply":"2024-11-07T16:20:12.714056Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Load the pretrained PEGASUS-XSum model and tokenizer\npegasus = \"google/pegasus-xsum\"\nmodel = PegasusForConditionalGeneration.from_pretrained(pegasus)\ntokenizer = PegasusTokenizer.from_pretrained(pegasus)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:20:31.564442Z","iopub.execute_input":"2024-11-07T16:20:31.565059Z","iopub.status.idle":"2024-11-07T16:20:38.472363Z","shell.execute_reply.started":"2024-11-07T16:20:31.564992Z","shell.execute_reply":"2024-11-07T16:20:38.471238Z"}},"outputs":[{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load the XSum dataset\ndataset = load_dataset(\"xsum\")\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:20:50.380288Z","iopub.execute_input":"2024-11-07T16:20:50.380751Z","iopub.status.idle":"2024-11-07T16:20:52.572926Z","shell.execute_reply.started":"2024-11-07T16:20:50.380712Z","shell.execute_reply":"2024-11-07T16:20:52.572011Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 204045\n    })\n    validation: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11332\n    })\n    test: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11334\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"test = dataset[\"test\"]\ntest","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:21:07.810573Z","iopub.execute_input":"2024-11-07T16:21:07.811475Z","iopub.status.idle":"2024-11-07T16:21:07.817835Z","shell.execute_reply.started":"2024-11-07T16:21:07.811431Z","shell.execute_reply":"2024-11-07T16:21:07.816862Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['document', 'summary', 'id'],\n    num_rows: 11334\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Set device and move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:21:27.181301Z","iopub.execute_input":"2024-11-07T16:21:27.182076Z","iopub.status.idle":"2024-11-07T16:21:27.201022Z","shell.execute_reply.started":"2024-11-07T16:21:27.182016Z","shell.execute_reply":"2024-11-07T16:21:27.200110Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Function to generate summaries using PEGASUS-XSum with error handling\ndef generate_summary_batch(texts):\n    try:\n        inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n\n        # Generate summaries\n        summaries = model.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_length=60,\n            num_beams=5,\n            length_penalty=2.0,\n            early_stopping=True\n        )\n        return tokenizer.batch_decode(summaries, skip_special_tokens=True)\n    except RuntimeError as e:\n        print(f\"Error during generation: {e}\")\n        return [\"\"] * len(texts)  # Return empty summaries in case of error","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:21:57.800866Z","iopub.execute_input":"2024-11-07T16:21:57.801305Z","iopub.status.idle":"2024-11-07T16:21:57.809003Z","shell.execute_reply.started":"2024-11-07T16:21:57.801264Z","shell.execute_reply":"2024-11-07T16:21:57.807891Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Function to compute ROUGE scores\ndef compute_rouge(predictions, references):\n    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n    rouge_results = {\n        \"rouge1\": [],\n        \"rouge2\": [],\n        \"rougeL\": []\n    }\n    for pred, ref in zip(predictions, references):\n        scores = scorer.score(ref, pred)\n        rouge_results[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n        rouge_results[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n        rouge_results[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n    return {metric: sum(scores) / len(scores) for metric, scores in rouge_results.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:22:13.460374Z","iopub.execute_input":"2024-11-07T16:22:13.460786Z","iopub.status.idle":"2024-11-07T16:22:13.468498Z","shell.execute_reply.started":"2024-11-07T16:22:13.460745Z","shell.execute_reply":"2024-11-07T16:22:13.467386Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Function to compute BLEU score\ndef compute_bleu(predictions, references):\n    pred_tokens = [nltk.word_tokenize(pred.lower()) for pred in predictions]\n    ref_tokens = [[nltk.word_tokenize(ref.lower())] for ref in references]\n    return corpus_bleu(ref_tokens, pred_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:22:24.195889Z","iopub.execute_input":"2024-11-07T16:22:24.196845Z","iopub.status.idle":"2024-11-07T16:22:24.202336Z","shell.execute_reply.started":"2024-11-07T16:22:24.196796Z","shell.execute_reply":"2024-11-07T16:22:24.201318Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Create DataLoader for batching\ndef create_dataloader(dataset, batch_size=8):\n    def collate_fn(batch):\n        texts = [example[\"document\"] for example in batch]\n        references = [example[\"summary\"] for example in batch]\n        return texts, references\n\n    return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:22:34.708188Z","iopub.execute_input":"2024-11-07T16:22:34.708584Z","iopub.status.idle":"2024-11-07T16:22:34.715117Z","shell.execute_reply.started":"2024-11-07T16:22:34.708547Z","shell.execute_reply":"2024-11-07T16:22:34.713962Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Set batch size for evaluation\nbatch_size = 8\ndataloader = create_dataloader(test, batch_size)\n\nall_generated_summaries = []\nall_references = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:22:57.352896Z","iopub.execute_input":"2024-11-07T16:22:57.353326Z","iopub.status.idle":"2024-11-07T16:22:57.358676Z","shell.execute_reply.started":"2024-11-07T16:22:57.353286Z","shell.execute_reply":"2024-11-07T16:22:57.357587Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Ensure model is in evaluation mode\nmodel.eval()\n\nfor batch in tqdm(dataloader, desc=\"Evaluating\"):\n    texts, references = batch\n    generated_summaries = generate_summary_batch(texts)\n    all_generated_summaries.extend(generated_summaries)\n    all_references.extend(references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T16:23:10.686828Z","iopub.execute_input":"2024-11-07T16:23:10.687350Z","iopub.status.idle":"2024-11-07T17:32:26.402770Z","shell.execute_reply.started":"2024-11-07T16:23:10.687307Z","shell.execute_reply":"2024-11-07T17:32:26.401703Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 1417/1417 [1:09:15<00:00,  2.93s/it]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Compute ROUGE scores\nrouge_scores = compute_rouge(all_generated_summaries, all_references)\n\n# Compute BLEU score\nbleu_score = compute_bleu(all_generated_summaries, all_references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T17:32:26.405190Z","iopub.execute_input":"2024-11-07T17:32:26.405902Z","iopub.status.idle":"2024-11-07T17:32:50.180302Z","shell.execute_reply.started":"2024-11-07T17:32:26.405851Z","shell.execute_reply":"2024-11-07T17:32:50.179218Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Print results\nprint(\"ROUGE Scores:\")\nfor metric, score in rouge_scores.items():\n    print(f\"{metric}: {score:.4f}\")\n\nprint(f\"BLEU Score: {bleu_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T17:32:50.181569Z","iopub.execute_input":"2024-11-07T17:32:50.181867Z","iopub.status.idle":"2024-11-07T17:32:50.187687Z","shell.execute_reply.started":"2024-11-07T17:32:50.181834Z","shell.execute_reply":"2024-11-07T17:32:50.186797Z"}},"outputs":[{"name":"stdout","text":"ROUGE Scores:\nrouge1: 0.4723\nrouge2: 0.2413\nrougeL: 0.3889\nBLEU Score: 0.1798\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets rouge_score nltk -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:30:59.975002Z","iopub.execute_input":"2024-11-07T18:30:59.975794Z","iopub.status.idle":"2024-11-07T18:31:11.416404Z","shell.execute_reply.started":"2024-11-07T18:30:59.975749Z","shell.execute_reply":"2024-11-07T18:31:11.415145Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # For more detailed CUDA error tracking\n\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom rouge_score import rouge_scorer\nimport nltk\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:31:11.418317Z","iopub.execute_input":"2024-11-07T18:31:11.418662Z","iopub.status.idle":"2024-11-07T18:31:16.012931Z","shell.execute_reply.started":"2024-11-07T18:31:11.418629Z","shell.execute_reply":"2024-11-07T18:31:16.011970Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Load the pretrained T5 model and tokenizer\nt5 = \"t5-base\"\nmodel = T5ForConditionalGeneration.from_pretrained(t5)\ntokenizer = T5Tokenizer.from_pretrained(t5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:31:45.698727Z","iopub.execute_input":"2024-11-07T18:31:45.699703Z","iopub.status.idle":"2024-11-07T18:31:55.708653Z","shell.execute_reply.started":"2024-11-07T18:31:45.699660Z","shell.execute_reply":"2024-11-07T18:31:55.707765Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ad2bdc22694ca393873990b234b453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46852f370167488a9369e7864db7b774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ec16fe8ae754ea497f06d7120f8fa74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ad46acc537745ef9c8d5b8718402d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"464fd9035cf04c3cb2a01c78641c52c2"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load the XSum dataset\ndataset = load_dataset(\"xsum\")\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:31:55.765401Z","iopub.execute_input":"2024-11-07T18:31:55.765681Z","iopub.status.idle":"2024-11-07T18:32:00.414297Z","shell.execute_reply.started":"2024-11-07T18:31:55.765649Z","shell.execute_reply":"2024-11-07T18:32:00.413373Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 204045\n    })\n    validation: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11332\n    })\n    test: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11334\n    })\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"test = dataset[\"test\"]\ntest","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:32:00.444020Z","iopub.execute_input":"2024-11-07T18:32:00.444330Z","iopub.status.idle":"2024-11-07T18:32:00.450310Z","shell.execute_reply.started":"2024-11-07T18:32:00.444297Z","shell.execute_reply":"2024-11-07T18:32:00.449423Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['document', 'summary', 'id'],\n    num_rows: 11334\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Set device and enable DataParallel for multiple GPUs\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs.\")\n    model = torch.nn.DataParallel(model)\nmodel.to(device)\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:32:16.510741Z","iopub.execute_input":"2024-11-07T18:32:16.511507Z","iopub.status.idle":"2024-11-07T18:32:16.990813Z","shell.execute_reply.started":"2024-11-07T18:32:16.511469Z","shell.execute_reply":"2024-11-07T18:32:16.989842Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Function to generate summaries using T5 with error handling\ndef generate_summary_batch(texts):\n    try:\n        inputs = tokenizer([\"summarize: \" + text for text in texts], return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n\n        # Generate summaries\n        summaries = model.module.generate(  # Use model.module for DataParallel models\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_length=60,\n            num_beams=5,\n            length_penalty=2.0,\n            early_stopping=True\n        )\n        return tokenizer.batch_decode(summaries, skip_special_tokens=True)\n    except RuntimeError as e:\n        print(f\"Error during generation: {e}\")\n        return [\"\"] * len(texts)  # Return empty summaries in case of error","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:32:32.536999Z","iopub.execute_input":"2024-11-07T18:32:32.537901Z","iopub.status.idle":"2024-11-07T18:32:32.544990Z","shell.execute_reply.started":"2024-11-07T18:32:32.537858Z","shell.execute_reply":"2024-11-07T18:32:32.543949Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Function to compute ROUGE scores\ndef compute_rouge(predictions, references):\n    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n    rouge_results = {\n        \"rouge1\": [],\n        \"rouge2\": [],\n        \"rougeL\": []\n    }\n    for pred, ref in zip(predictions, references):\n        scores = scorer.score(ref, pred)\n        rouge_results[\"rouge1\"].append(scores[\"rouge1\"].fmeasure)\n        rouge_results[\"rouge2\"].append(scores[\"rouge2\"].fmeasure)\n        rouge_results[\"rougeL\"].append(scores[\"rougeL\"].fmeasure)\n    return {metric: sum(scores) / len(scores) for metric, scores in rouge_results.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:32:33.124722Z","iopub.execute_input":"2024-11-07T18:32:33.125025Z","iopub.status.idle":"2024-11-07T18:32:33.131892Z","shell.execute_reply.started":"2024-11-07T18:32:33.124994Z","shell.execute_reply":"2024-11-07T18:32:33.130836Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Function to compute BLEU score\ndef compute_bleu(predictions, references):\n    pred_tokens = [nltk.word_tokenize(pred.lower()) for pred in predictions]\n    ref_tokens = [[nltk.word_tokenize(ref.lower())] for ref in references]\n    return corpus_bleu(ref_tokens, pred_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:32:33.748412Z","iopub.execute_input":"2024-11-07T18:32:33.748732Z","iopub.status.idle":"2024-11-07T18:32:33.753579Z","shell.execute_reply.started":"2024-11-07T18:32:33.748700Z","shell.execute_reply":"2024-11-07T18:32:33.752540Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Create DataLoader for batching\ndef create_dataloader(dataset, batch_size=8):\n    def collate_fn(batch):\n        texts = [example[\"document\"] for example in batch]\n        references = [example[\"summary\"] for example in batch]\n        return texts, references\n\n    return DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:32:34.380786Z","iopub.execute_input":"2024-11-07T18:32:34.381112Z","iopub.status.idle":"2024-11-07T18:32:34.386921Z","shell.execute_reply.started":"2024-11-07T18:32:34.381076Z","shell.execute_reply":"2024-11-07T18:32:34.386011Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Set batch size for evaluation\nbatch_size = 8\ndataloader = create_dataloader(test, batch_size)\n\nall_generated_summaries = []\nall_references = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:32:35.992314Z","iopub.execute_input":"2024-11-07T18:32:35.992681Z","iopub.status.idle":"2024-11-07T18:32:35.997715Z","shell.execute_reply.started":"2024-11-07T18:32:35.992645Z","shell.execute_reply":"2024-11-07T18:32:35.996807Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Ensure model is in evaluation mode\nmodel.eval()\n\nfor batch in tqdm(dataloader, desc=\"Evaluating\"):\n    texts, references = batch\n    generated_summaries = generate_summary_batch(texts)\n    all_generated_summaries.extend(generated_summaries)\n    all_references.extend(references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T18:32:36.783289Z","iopub.execute_input":"2024-11-07T18:32:36.783616Z","iopub.status.idle":"2024-11-07T20:09:41.640727Z","shell.execute_reply.started":"2024-11-07T18:32:36.783584Z","shell.execute_reply":"2024-11-07T20:09:41.639642Z"}},"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 1417/1417 [1:37:04<00:00,  4.11s/it]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Compute ROUGE scores\nrouge_scores = compute_rouge(all_generated_summaries, all_references)\n\n# Compute BLEU score\nbleu_score = compute_bleu(all_generated_summaries, all_references)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T20:09:41.642560Z","iopub.execute_input":"2024-11-07T20:09:41.642914Z","iopub.status.idle":"2024-11-07T20:10:16.280890Z","shell.execute_reply.started":"2024-11-07T20:09:41.642874Z","shell.execute_reply":"2024-11-07T20:10:16.280105Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Print results\nprint(\"ROUGE Scores:\")\nfor metric, score in rouge_scores.items():\n    print(f\"{metric}: {score:.4f}\")\n\nprint(f\"BLEU Score: {bleu_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T20:10:16.293125Z","iopub.execute_input":"2024-11-07T20:10:16.293445Z","iopub.status.idle":"2024-11-07T20:10:16.298960Z","shell.execute_reply.started":"2024-11-07T20:10:16.293392Z","shell.execute_reply":"2024-11-07T20:10:16.298097Z"}},"outputs":[{"name":"stdout","text":"ROUGE Scores:\nrouge1: 0.2050\nrouge2: 0.0309\nrougeL: 0.1385\nBLEU Score: 0.0129\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}